{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing the Required Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nfrom skimage import io\nimport torch.utils.data as data\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport glob\nfrom PIL import Image\nfrom torch.utils.data.sampler import BatchSampler\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nfrom skimage.io import imread, imshow\nimport cv2\nfrom torch.optim import lr_scheduler\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\ncuda = torch.cuda.is_available()\ntorch.manual_seed(42)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T20:24:50.482020Z","iopub.execute_input":"2022-01-27T20:24:50.482702Z","iopub.status.idle":"2022-01-27T20:24:50.495590Z","shell.execute_reply.started":"2022-01-27T20:24:50.482663Z","shell.execute_reply":"2022-01-27T20:24:50.494911Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# Making custom dataset class glvd to extract the dataset and make Triplets\nWe have taken 250 classes at once for our evaluation purpose as the dataset was quite big and it was computationally expensive","metadata":{}},{"cell_type":"code","source":"class glvd(Dataset):\n    \n    def __init__(self, train=True, transform=None):\n        self.transform = transform\n        self.train = train\n        self.df = pd.read_csv(\"../input/landmark-retrieval-2021/train.csv\")\n        self.df = self.df[self.df['landmark_id']<=1000]\n        if self.train:\n            self.grouped = self.df.groupby(self.df['landmark_id'])\n        else:\n            #self.testf = [f for f in glob.glob(\"../input/landmark-retrieval-2021/test/\"+'*/*/*/*')]\n            self.testf = self.df.sample(n=500).reset_index(drop=True)\n    def __len__(self):\n        if self.train:\n            return len(self.df)\n        else:\n            return len(self.testf)\n    \n    def __getitem__(self, idx):\n        #Taking the data and making triplets of the train data and returning query image for test\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n            \n        if self.train:\n            pos_grp = self.grouped.get_group(self.df['landmark_id'].iloc[idx])\n            neg_grp = pd.concat([self.df, pos_grp]).drop_duplicates(keep=False).reset_index(drop=True)\n            \n            ahash = self.df['id'].iloc[idx]\n            phash = pos_grp.sample()['id'].iloc[0]\n            nhash = neg_grp.sample()['id'].iloc[0]\n            \n            Anchor = io.imread(\"../input/landmark-retrieval-2021/train/\"+ahash[0]+\"/\"+ahash[1]+\"/\"+ahash[2]+\"/\"+ahash+\".jpg\")\n            Positive = io.imread(\"../input/landmark-retrieval-2021/train/\"+phash[0]+\"/\"+phash[1]+\"/\"+phash[2]+\"/\"+phash+\".jpg\")\n            Negative = io.imread(\"../input/landmark-retrieval-2021/train/\"+nhash[0]+\"/\"+nhash[1]+\"/\"+nhash[2]+\"/\"+nhash+\".jpg\")\n            \n            if self.transform is not None:\n                Anchor = self.transform(Anchor)\n                Positive = self.transform(Positive)\n                Negative = self.transform(Negative)\n            return (Anchor, Positive, Negative),[]\n        else:\n            qhash = self.testf['id'].iloc[idx]\n            query = io.imread(\"../input/landmark-retrieval-2021/train/\"+qhash[0]+\"/\"+qhash[1]+\"/\"+qhash[2]+\"/\"+qhash+\".jpg\")\n            if self.transform is not None:\n                    query = self.transform(query)\n            return query\n        \n    def hashedvd(self,idx):\n        #function to take the hashed format of image to print the query image in original form while retrieving\n        return self.testf['id'].iloc[idx]\n            ","metadata":{"execution":{"iopub.status.busy":"2022-01-27T20:24:50.497239Z","iopub.execute_input":"2022-01-27T20:24:50.497758Z","iopub.status.idle":"2022-01-27T20:24:50.514458Z","shell.execute_reply.started":"2022-01-27T20:24:50.497721Z","shell.execute_reply":"2022-01-27T20:24:50.513747Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# Making custom dataset glvdk to extract the training data in form of a Tensor for making embeddings for Image Retrieval","metadata":{}},{"cell_type":"code","source":"class glvdk(Dataset):\n    \n    def __init__(self, transform=None):\n        self.transform = transform\n        self.df = pd.read_csv(\"../input/landmark-retrieval-2021/train.csv\")\n        self.df = self.df[self.df['landmark_id']<=250]\n        \n    def __len__(self):\n        return len(self.df)\n        \n            \n    \n    def __getitem__(self, idx):\n        \n        if torch.is_tensor(idx):\n            idx = idx.tolist()    \n        ahash = self.df['id'].iloc[idx]   \n        Anchor = io.imread(\"../input/landmark-retrieval-2021/train/\"+ahash[0]+\"/\"+ahash[1]+\"/\"+ahash[2]+\"/\"+ahash+\".jpg\")\n            \n            \n        if self.transform is not None:\n                Anchor = self.transform(Anchor)\n                \n        return Anchor,self.df['landmark_id'].iloc[idx] #returns training data with image tensor and labels","metadata":{"execution":{"iopub.status.busy":"2022-01-27T20:24:50.515865Z","iopub.execute_input":"2022-01-27T20:24:50.516397Z","iopub.status.idle":"2022-01-27T20:24:50.525776Z","shell.execute_reply.started":"2022-01-27T20:24:50.516357Z","shell.execute_reply":"2022-01-27T20:24:50.525094Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"#transforms to be applied on our data \ntrnsfm = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((224,224))\n ])","metadata":{"execution":{"iopub.status.busy":"2022-01-27T20:24:50.529282Z","iopub.execute_input":"2022-01-27T20:24:50.529720Z","iopub.status.idle":"2022-01-27T20:24:50.533993Z","shell.execute_reply.started":"2022-01-27T20:24:50.529685Z","shell.execute_reply":"2022-01-27T20:24:50.533254Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# Part 1:Model Creation and Training:-","metadata":{}},{"cell_type":"markdown","source":"# Making the Triplet Loss Function\nTriplet Loss is the Loss Function which makes sure that Given an Anchor Image A is closer to its positive image P and farther from its negative Image N so that the model maps the embeddings to similar classes\nReferences:-[Triplet Loss](https://medium.com/analytics-vidhya/triplet-loss-b9da35be21b8)","metadata":{}},{"cell_type":"code","source":"class TripletLoss(nn.Module):\n    def __init__(self, margin):\n        super(TripletLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, anchor, positive, negative, size_average=True):\n        #Triplet Loss=pos_distance-neg_distance+margin\n        distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n        distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n        losses = F.relu(distance_positive - distance_negative + self.margin)\n        return losses.mean() if size_average else losses.sum()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-27T20:24:50.535254Z","iopub.execute_input":"2022-01-27T20:24:50.535561Z","iopub.status.idle":"2022-01-27T20:24:50.542185Z","shell.execute_reply.started":"2022-01-27T20:24:50.535526Z","shell.execute_reply":"2022-01-27T20:24:50.541514Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"# Making an Embedding Net to Convert Image to a Lower Dimensional Embedding for Computation","metadata":{}},{"cell_type":"code","source":"class EmbeddingNet(nn.Module):\n    def __init__(self):\n        #constructing an Embedding Network \n        super(EmbeddingNet, self).__init__()\n        self.convnet=torchvision.models.resnet18(pretrained=True)\n        self.convnet.fc = nn.Linear(self.convnet.fc.in_features, 2)\n\n    def forward(self, x):\n        output = self.convnet(x)\n        return output\n\n    def get_embedding(self, x):\n        return self.forward(x)\n\n\nclass EmbeddingNetL2(EmbeddingNet):\n    #Applying L2 Normalisation to our Embedding Net\n    def __init__(self):\n        super(EmbeddingNetL2, self).__init__()\n\n    def forward(self, x):\n        output = super(EmbeddingNetL2, self).forward(x)\n        output /= output.pow(2).sum(1, keepdim=True).sqrt()\n        return output\n\n    def get_embedding(self, x):\n        return self.forward(x)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T20:24:50.543556Z","iopub.execute_input":"2022-01-27T20:24:50.544045Z","iopub.status.idle":"2022-01-27T20:24:50.553872Z","shell.execute_reply.started":"2022-01-27T20:24:50.544006Z","shell.execute_reply":"2022-01-27T20:24:50.553226Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"# Constructing a Triplet Network which maps our embeddings to similar classes \nTriplet Network basically minimizes the distance between Anchor and Positive and maximizes the distance between Anchor and Negative thus mapping our embeddings to similar classes\nWe were first working on Siamese Network and implemented it in earlier baseline but we found that Triplet Net,its immediate competitor had a better training accuracy as it had three channels for A,P,N compared to Siamese which had 2(one for Anchor and other one for transferring in Pairs of positive and negative)","metadata":{}},{"cell_type":"code","source":"class TripletNet(nn.Module):\n    def __init__(self, embedding_net):\n        super(TripletNet, self).__init__()\n        self.embedding_net = embedding_net\n\n    def forward(self, x1, x2, x3):\n        output1 = self.embedding_net(x1)\n        output2 = self.embedding_net(x2)\n        output3 = self.embedding_net(x3)\n        return output1, output2, output3\n\n    def get_embedding(self, x):\n        return self.embedding_net(x)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T20:24:50.557000Z","iopub.execute_input":"2022-01-27T20:24:50.557439Z","iopub.status.idle":"2022-01-27T20:24:50.563910Z","shell.execute_reply.started":"2022-01-27T20:24:50.557411Z","shell.execute_reply":"2022-01-27T20:24:50.563239Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"#importing our train dataset in A,P,N and test dataset as Query Images\ntrain_dataset = glvd(transform=trnsfm)\ntest_dataset = glvd(train=False, transform=trnsfm)\nn_classes = 250","metadata":{"execution":{"iopub.status.busy":"2022-01-27T20:24:50.735275Z","iopub.execute_input":"2022-01-27T20:24:50.736221Z","iopub.status.idle":"2022-01-27T20:24:52.571309Z","shell.execute_reply.started":"2022-01-27T20:24:50.736170Z","shell.execute_reply":"2022-01-27T20:24:52.570584Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"#Splitting our dataset as train and Validation\ntrain_set_size = int(len(train_dataset) * 0.8)\nvalid_set_size = len(train_dataset) - train_set_size\ntrain_dataset, valid_dataset = data.random_split(train_dataset, [train_set_size, valid_set_size])","metadata":{"execution":{"iopub.status.busy":"2022-01-27T20:24:52.573439Z","iopub.execute_input":"2022-01-27T20:24:52.573832Z","iopub.status.idle":"2022-01-27T20:24:52.579526Z","shell.execute_reply.started":"2022-01-27T20:24:52.573796Z","shell.execute_reply":"2022-01-27T20:24:52.578881Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def fit(train_loader, val_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics=[],\n        start_epoch=0):\n    \n    for epoch in range(0, start_epoch):\n        scheduler.step()\n\n    for epoch in range(start_epoch, n_epochs):\n        \n        train_loss, metrics = train_epoch(train_loader, model, loss_fn, optimizer, cuda, log_interval, metrics)\n        \n        scheduler.step()\n\n        # Train stage\n        \n\n        message = 'Epoch: {}/{}. Train set: Average loss: {:.4f}'.format(epoch + 1, n_epochs, train_loss)\n        for metric in metrics:\n            message += '\\t{}: {}'.format(metric.name(), metric.value())\n\n        val_loss, metrics = test_epoch(val_loader, model, loss_fn, cuda, metrics)\n        val_loss /= len(val_loader)\n\n        message += '\\nEpoch: {}/{}. Validation set: Average loss: {:.4f}'.format(epoch + 1, n_epochs,\n                                                                                 val_loss)\n        for metric in metrics:\n            message += '\\t{}: {}'.format(metric.name(), metric.value())\n\n        print(message)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T20:24:52.580837Z","iopub.execute_input":"2022-01-27T20:24:52.581098Z","iopub.status.idle":"2022-01-27T20:24:52.591029Z","shell.execute_reply.started":"2022-01-27T20:24:52.581065Z","shell.execute_reply":"2022-01-27T20:24:52.590376Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"def train_epoch(train_loader, model, loss_fn, optimizer, cuda, log_interval, metrics):\n    for metric in metrics:\n        metric.reset()\n\n    model.train()\n    losses = []\n    total_loss = 0\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        target = target if len(target) > 0 else None\n        if not type(data) in (tuple, list):\n            data = (data,)\n        if cuda:\n            data = tuple(d.cuda() for d in data)\n            if target is not None:\n                target = target.cuda()\n\n\n        optimizer.zero_grad()\n        outputs = model(*data)\n\n        if type(outputs) not in (tuple, list):\n            outputs = (outputs,)\n\n        loss_inputs = outputs\n        if target is not None:\n            target = (target,)\n            loss_inputs += target\n\n        loss_outputs = loss_fn(*loss_inputs)\n        loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n        losses.append(loss.item())\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n        for metric in metrics:\n            metric(outputs, target, loss_outputs)\n\n        if batch_idx % log_interval == 0:\n            \n            message = 'Train: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                batch_idx * len(data[0]), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), np.mean(losses))\n            for metric in metrics:\n                message += '\\t{}: {}'.format(metric.name(), metric.value())\n\n            print(message)\n            losses = []\n\n    total_loss /= (batch_idx + 1)\n    return total_loss, metrics","metadata":{"execution":{"iopub.status.busy":"2022-01-27T20:24:52.592462Z","iopub.execute_input":"2022-01-27T20:24:52.592948Z","iopub.status.idle":"2022-01-27T20:24:52.605433Z","shell.execute_reply.started":"2022-01-27T20:24:52.592913Z","shell.execute_reply":"2022-01-27T20:24:52.604725Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def test_epoch(val_loader, model, loss_fn, cuda, metrics):\n    with torch.no_grad():\n        for metric in metrics:\n            metric.reset()\n        model.eval()\n        val_loss = 0\n        for batch_idx, (data, target) in enumerate(val_loader):\n            target = target if len(target) > 0 else None\n            if not type(data) in (tuple, list):\n                data = (data,)\n            if cuda:\n                data = tuple(d.cuda() for d in data)\n                if target is not None:\n                    target = target.cuda()\n\n            outputs = model(*data)\n\n            if type(outputs) not in (tuple, list):\n                outputs = (outputs,)\n            loss_inputs = outputs\n            if target is not None:\n                target = (target,)\n                loss_inputs += target\n\n            loss_outputs = loss_fn(*loss_inputs)\n            loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n            val_loss += loss.item()\n\n            for metric in metrics:\n                metric(outputs, target, loss_outputs)\n\n    return val_loss, metrics","metadata":{"execution":{"iopub.status.busy":"2022-01-27T20:24:52.608144Z","iopub.execute_input":"2022-01-27T20:24:52.608687Z","iopub.status.idle":"2022-01-27T20:24:52.618330Z","shell.execute_reply.started":"2022-01-27T20:24:52.608649Z","shell.execute_reply":"2022-01-27T20:24:52.617613Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"#functions to extract embeddings of train data and test data\ndef extract_embeddings(dataloader, model):\n    with torch.no_grad():\n        model.eval()\n        embeddings = np.zeros((len(dataloader.dataset), 2))\n        labels = np.zeros(len(dataloader.dataset))\n        k = 0\n        for images, target in dataloader:\n            if cuda:\n                images = images.cuda()\n            embeddings[k:k+len(images)] = model.get_embedding(images).data.cpu().numpy()\n            labels[k:k+len(images)] = target.numpy()\n            k += len(images)\n    return embeddings, labels\n\n\n\ndef extract_embeddings_test(dataloader, model):\n    with torch.no_grad():\n        model.eval()\n        embeddings = np.zeros((len(dataloader.dataset), 2))\n        k = 0\n        for images in dataloader:\n            if cuda:\n                images = images.cuda()\n            embeddings[k:k+len(images)] = model.get_embedding(images).data.cpu().numpy()\n            k += len(images)\n    return embeddings","metadata":{"execution":{"iopub.status.busy":"2022-01-27T20:24:52.619466Z","iopub.execute_input":"2022-01-27T20:24:52.619835Z","iopub.status.idle":"2022-01-27T20:24:52.629635Z","shell.execute_reply.started":"2022-01-27T20:24:52.619801Z","shell.execute_reply":"2022-01-27T20:24:52.628956Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"\nbatch_size = 8\nkwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\ntriplet_train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\ntriplet_test_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n\n# Set up the network and training parameters(Hyperparameters Tuning)\nmargin = 1.\nembedding_net = EmbeddingNet()\nmodel = TripletNet(embedding_net)\nif cuda:\n    model.cuda()\nloss_fn = TripletLoss(margin)\nlr = 1e-3\noptimizer = optim.Adam(model.parameters(), lr=lr)\nscheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\nn_epochs = 15\nlog_interval = 100","metadata":{"execution":{"iopub.status.busy":"2022-01-27T20:24:52.630881Z","iopub.execute_input":"2022-01-27T20:24:52.631304Z","iopub.status.idle":"2022-01-27T20:24:52.864709Z","shell.execute_reply.started":"2022-01-27T20:24:52.631257Z","shell.execute_reply":"2022-01-27T20:24:52.863992Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"fit(triplet_train_loader, triplet_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T20:24:52.866107Z","iopub.execute_input":"2022-01-27T20:24:52.866395Z","iopub.status.idle":"2022-01-27T21:32:57.233078Z","shell.execute_reply.started":"2022-01-27T20:24:52.866359Z","shell.execute_reply":"2022-01-27T21:32:57.232244Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"# Prt 2:Extracting Embeddings of our Train and Test Data:-","metadata":{}},{"cell_type":"code","source":"#importing train dataset from glvdk for embeddings formation after evaluation and mapping from knn\ntrain_datasetk=glvdk(transform=trnsfm)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T21:32:57.234980Z","iopub.execute_input":"2022-01-27T21:32:57.235452Z","iopub.status.idle":"2022-01-27T21:32:58.247229Z","shell.execute_reply.started":"2022-01-27T21:32:57.235406Z","shell.execute_reply":"2022-01-27T21:32:58.246357Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"batch_size = 256\nkwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\ntrain_loader = torch.utils.data.DataLoader(train_datasetk, batch_size=batch_size, shuffle=True, **kwargs)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, **kwargs)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T21:32:58.249200Z","iopub.execute_input":"2022-01-27T21:32:58.249747Z","iopub.status.idle":"2022-01-27T21:32:58.256094Z","shell.execute_reply.started":"2022-01-27T21:32:58.249705Z","shell.execute_reply":"2022-01-27T21:32:58.255324Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"train_embeddings_ocl, train_labels_ocl = extract_embeddings(train_loader, model)\ntest_embeddings_ocl = extract_embeddings_test(test_loader, model)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T21:32:58.257562Z","iopub.execute_input":"2022-01-27T21:32:58.257863Z","iopub.status.idle":"2022-01-27T21:33:26.731892Z","shell.execute_reply.started":"2022-01-27T21:32:58.257823Z","shell.execute_reply":"2022-01-27T21:33:26.731025Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"# Part 3:Image Retrieval","metadata":{}},{"cell_type":"code","source":"#transforms for Image Retrieval\ntrnsfmk=transforms.Compose([\n    transforms.ToTensor()\n    \n ])","metadata":{"execution":{"iopub.status.busy":"2022-01-27T21:33:26.733659Z","iopub.execute_input":"2022-01-27T21:33:26.733928Z","iopub.status.idle":"2022-01-27T21:33:26.737955Z","shell.execute_reply.started":"2022-01-27T21:33:26.733888Z","shell.execute_reply":"2022-01-27T21:33:26.737319Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"#importing train dataset for Image retrieval\ntrain_datasetr=glvdk(transform=trnsfmk)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T21:33:26.739232Z","iopub.execute_input":"2022-01-27T21:33:26.739705Z","iopub.status.idle":"2022-01-27T21:33:27.699651Z","shell.execute_reply.started":"2022-01-27T21:33:26.739670Z","shell.execute_reply":"2022-01-27T21:33:27.698940Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"num_test_queries = 1\nnum_elements_to_retrieve=5\nqhash=test_dataset.hashedvd(80)\nQ=io.imread(\"../input/landmark-retrieval-2021/train/\"+qhash[0]+\"/\"+qhash[1]+\"/\"+qhash[2]+\"/\"+qhash+\".jpg\")\nprint(\"Query:\")\nplt.imshow(Q)\nplt.show()\nprint(\"Retrieved Image:\")\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(train_embeddings_ocl, train_labels_ocl)\nindexes = knn.kneighbors(test_embeddings_ocl[80:81],num_elements_to_retrieve, return_distance=False)\nfor index in indexes:\n        print(train_labels_ocl[index])\n        n=train_labels_ocl[index[0]]\n        for instance,labels in train_datasetr:\n            if(n==labels):\n                plt.imshow(instance.permute(1,2,0))\n                plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-27T21:33:27.702453Z","iopub.execute_input":"2022-01-27T21:33:27.702803Z","iopub.status.idle":"2022-01-27T21:34:11.846023Z","shell.execute_reply.started":"2022-01-27T21:33:27.702756Z","shell.execute_reply":"2022-01-27T21:34:11.845272Z"},"trusted":true},"execution_count":52,"outputs":[]}]}